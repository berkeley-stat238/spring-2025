{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f237fc-0634-4962-bf04-babeefd6bb9a",
   "metadata": {},
   "source": [
    "# Lab 7: Bayesian Hierarchical Modeling\n",
    "\n",
    "### Lab Date: Wednesday, Mar 12\n",
    "\n",
    "### Lab Due: Wednesday, Apr 2\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Work with your lab group to complete the following notebook. Your work will be reviewed by your peers in three weeks (Wednesday, April 2nd)\n",
    "\n",
    "In this lab, you will:\n",
    "1. Understand a Bayesian hierarchical model in the context of image reconstruction.\n",
    "2. Apply the Fourier transform as a transformation matrix in an inverse problem.\n",
    "3. Implement Maximum A Posteriori (MAP) estimation using:\n",
    "\t- Coordinate Ascent Optimization\n",
    "\t- Gradient-Based Optimization (L-BFGS-B)\n",
    "4. Compute and interpret Laplace Approximation to estimate posterior uncertainty.\n",
    "5. Compare the effectiveness of different optimization methods in Bayesian inference.\n",
    "\n",
    "This lab applies Bayesian sparse regression to the MNIST dataset, where images of handwritten digits are processed and reconstructed using probabilistic models.\n",
    "\n",
    "If you are new to working in python, or in a Jupyter notebook, please ask your lab members for help. If you notice a lab member struggling, and have experience, please offer your help.\n",
    "\n",
    "Please see this [Ed post](https://edstem.org/us/courses/74615/discussion/6352659) for corrections, questions, and discussion. If you would rather work with your own copy of the files, I have uploaded a zip folder there with the lab materials. \n",
    "\n",
    "Corrections to the lab will be pushed directly to this notebook. We will only push corrections to the text, which is set to read only to prevent merge conflicts. In the event of a merge conflict, save your notebook under a different name, and click the link that launches the lab from the schedule on the [stat238 homepage](https://stat238.berkeley.edu/spring-2025/) again. Then, check for discrepancies. If you can't find them, or resolve the conflict, contact us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc7594-39e8-4a36-930a-67a8089de520",
   "metadata": {},
   "source": [
    "## Set Up:\n",
    "\n",
    "Run the cells below to load necessary packages and to load the example data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0c3e4-c0ee-4d80-b41d-c0c3d1e0b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If torchvision does not load for you, uncomment the line below and install torchvision\n",
    "# %pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets \n",
    "import scipy\n",
    "from scipy.linalg import inv\n",
    "from scipy.stats import invgamma\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88458e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an MNIST digit as an example\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((28, 28))])\n",
    "mnist_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "# Select an image from the dataset\n",
    "img, label = mnist_dataset[0]  # Get the first image\n",
    "img_array = img.squeeze().numpy()  # Convert from PyTorch tensor to NumPy\n",
    "x_true = img_array.flatten()  # Flatten the image for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ce455",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Image reconstruction is a common problem in fields such as medical, geophysical, and astronomical imaging. In this lab, we will explore an application of a Bayesian hierarchical model to reconstruct an image that has been blurred by a low-pass filter (a filter that damps high-frequency components of the image) and that has been corrupted by noise.\n",
    "\n",
    "We assume that the true image $x$ is transformed by an operator $A$, and we observe $\\tilde{x}$:\n",
    "\n",
    "$$\\tilde{x} = A x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$$\n",
    "\n",
    "where $A$ represents the low-pass filter. \n",
    "\n",
    "Any low pass filter can be decomposed $A = F^{*} D F$ where $F$ is a Fourier matrix (the matrix that implements a discrete Fourier transform), and $D$ is a diagonal matrix with decreasing non-negative entries. Since the discrete Fourier transform is orthonormal, its conjugate transpose, $F^{*}$, is $F^{-1}$, so $D$ is the matrix of eigenvalues of the low-pass filter, and $F$ contains its eigenvectors. Since $F$ is unitary, $\\zeta = F \\epsilon \\sim \\mathcal{N}(0,\\sigma^2 F F^*) = \\mathcal{N}(0,\\sigma^2 I)$. \n",
    "\n",
    "Since most signal processing pipelines begin by converting the original received signal, $\\tilde{x}$, to its Fourier transform, $y = F \\tilde{x}$, we can assume that the problem takes the form:\n",
    "\n",
    "$$ \\text{Estimate  } x \\text{  given: } \\quad y = A x + \\zeta, \\quad A = D F, \\quad \\quad \\zeta \\sim \\mathcal{N}(0, \\sigma^2 I).$$\n",
    "\n",
    "To recover $x$, we will try a Bayesian hierarchical model. First, we assume that the elements of $x$ are exchangeable, so they may be modeled as conditionally i.i.d. Here, we assume that the entries of the true image are, conditional on an unknown scale parameter, drawn i.i.d. from a Gaussian prior:\n",
    "\n",
    "$$x_i \\sim \\mathcal{N}(0, \\tau^2)$$\n",
    "\n",
    "We place an Inverse-Gamma hyperprior on $\\tau^2$:\n",
    "\n",
    "$$\\tau^2 \\sim \\text{Inv-Gamma}(\\alpha, \\beta)$$\n",
    "\n",
    "Our goal is to estimate $x$ using MAP estimation and compare different optimization methods. We will estimate $x$ and $\\tau$ jointly. This will, in effect, place an $L_2$ regularization on the image recovery problem where the regularization parameter is unknown and optimized in tandem with the image $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e90aa-e584-4a8e-a5e1-fd507d05c653",
   "metadata": {},
   "source": [
    "## Part I: Model Construction\n",
    "\n",
    "With your group, make a diagram representing the hierarchical model. Then, answer the questions below.\n",
    "\n",
    "**Q 1.1:** Find the marginal prior distribution for each $x_i$ by marginalizing over $\\tau^2$. Show that this mixture produces a standard distribution. Hint: it will help to know that, an inverse-gamma distributed random variable is the reciprocal of a Gamma distributed random variable with the same parameters. What does this result suggest about the regularizing effect of the hierarchical model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc661b-2d74-4b28-8d41-93f5ed841215",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc648874-b3a8-4476-bbde-1ad8b85cc98f",
   "metadata": {},
   "source": [
    "**Q 1.2:** Under this model, are the entries of the true image i.i.d.? Are they correlated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3da571-1cc7-4802-a473-03c658b6514a",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85669d7f-42cf-4ffd-a9e7-75a76a110093",
   "metadata": {},
   "source": [
    "**Q 1.3:** Write a code that can sample $x_1, x_2$ from the hierarchical model. Produce 1,000 samples, $\\{x_1(i),x_2(i)\\}_{i=1}^{1,000}$, and make a scatter plot showing the position of the samples. Make sure to match the aspect ratio of the axes in your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3bf95-8243-424e-97b0-8f1c520bf9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your sampling code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ae587-11dc-4f89-8106-6de0aa09de74",
   "metadata": {},
   "source": [
    "**Q 1.4:** Note any patterns you observe in the scatter plot. In particular, comment on how $x_1$ and $x_2$ are coupled by the hierarchical model. In what sense do they share information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c19948-dba5-453d-893f-fbbfe5a1f7e6",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4bce82-b27f-46c1-88ed-5b55ab718461",
   "metadata": {},
   "source": [
    "**Q 1.5:** Suppose we modified the hierarchical model so that, instead of drawing a global scale parameter $\\tau^2$, shared by all $x_i$, we drew a scale for each $x_i$, $\\tau_i^2$, i.i.d. from an inverse gamma with parameters $\\alpha$ and $\\beta$. This is sometimes called \"local regularization\". Sketch the ensuing hierarchical model, then discuss how switching from global to local regularization would change the way information is shared across the model, and change the character of solutions promoted by the prior if we optimize over $x$ and $\\tau$ jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c1496-eb12-4c82-8078-d8eafc73c2ed",
   "metadata": {},
   "source": [
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf39332-3cf8-418e-9d8d-a6f683e369d6",
   "metadata": {},
   "source": [
    "### Construct the Low-Pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1fa1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_transform_matrix(size):\n",
    "    \"\"\"Create a Fourier transformation matrix.\"\"\"\n",
    "    F1 = scipy.linalg.dft(size)/np.sqrt(size) # Build orthonormal discrete-Fourier transform matrix in one dimension\n",
    "    F2 = np.kron(F1, F1) # extend to the two-dimensional transform\n",
    "    return F2\n",
    "\n",
    "# Define the low-pass filter\n",
    "def low_pass_filter(size):\n",
    "    \"\"\"Create the low pass filter.\"\"\"\n",
    "    d = 10**np.linspace(0,-1.5,size) # notice, this is chosen so that the eigenvalues of A cover many orders of magnitude\n",
    "    D1 = np.diag(d)\n",
    "    D = np.kron(D1,D1)\n",
    "    F = fourier_transform_matrix(size)\n",
    "    A = D @ F\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191c365-1330-4c70-9300-05fdf3fbb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transforms\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "F = fourier_transform_matrix(12)\n",
    "A = low_pass_filter(12)\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(F.real, cmap=\"gray\")\n",
    "plt.title(\"Fourier Matrix (real)\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(F.real, cmap=\"gray\")\n",
    "plt.title(\"Fourier Matrix (imag)\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(A.real, cmap=\"gray\")\n",
    "plt.title(\"Filter (real)\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(A.real, cmap=\"gray\")\n",
    "plt.title(\"Filter (imag)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910cd9a",
   "metadata": {},
   "source": [
    "### Simulate the Observations\n",
    "1. Select an MNIST image and flatten it into a vector $x_{\\text{true}}$.\n",
    "2. Create the transformation matrix $A$.\n",
    "3. Generate observations by applying $A$ to $x_{\\text{true}}$, and then by adding noise to the transformed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise level\n",
    "noise_std = 0.05  # Standard deviation of Gaussian noise\n",
    "\n",
    "# Apply the transformation matrix and add noise\n",
    "A = low_pass_filter(len(x_true))\n",
    "y = A @ x_true + np.random.normal(0, noise_std, size=x_true.shape)  # Noisy observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d0085",
   "metadata": {},
   "source": [
    "## Part II: Point Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fb4ee-2b86-4104-be4e-bbdafd0ce4bd",
   "metadata": {},
   "source": [
    "To try and recover $x$, from $y$, we will try to find the joint MAP estimator $\\{\\hat{x}(y),\\hat{\\tau}^2(y)\\}$.\n",
    "\n",
    "**Q 2.1:** With your group members discuss whether the joint MAP problem, find $\\{\\hat{x}(y),\\hat{\\tau}^2(y)\\}$ that maximize $p_{x,\\tau|y}$, is distinct from the marginal MAP problem, find $\\{\\hat{x}(y)\\}$ that maximize $p_{x|y}$. Does the choice to maximize with, or marginalize over, the nuisance parameter $\\tau$ change the nature of the estimator? Comment on which of these two procedures would be easier to implement computationally for general hierarchical models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c67d5-e8df-4e3f-806d-7b6ef3b7a467",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379beed4-459e-435f-a381-78dd8aeb2dc8",
   "metadata": {},
   "source": [
    "### Coordinate-Ascent\n",
    "\n",
    "First, we will consider a coordinate ascent scheme. Coordinate ascent is an optimization technique that iteratively optimizes distinct subsets of the variables while keeping others fixed. For example, we might optimize $x_1$ holding all other variables fixed, then optimize $x_2$ holding all other variables fixed, and so on. The general algorithm is available in https://en.wikipedia.org/wiki/Coordinate_descent.\n",
    "\n",
    "Coordinate ascent is commonly used in problems where the full optimization problem is hard to solve directly but can be broken down into simpler subproblems. This is often the case for hierarchical models built out of tractable components. For example, it is common to use conditionally Gaussian hierarchical models, with independent elementwise priors on the parameters of the Gaussian. Then, each step that pools over related variables can be optimized in closed form via the linear algebra of the normal, normal model. The remaining variables may be optimized independently, in parallel. This breaks the problem into a recursive procedure that solves a sequence of easy optimize problems (e.g. least squares for pooling information in multiple dimensions, then a series of one-dimensional optimization problems). This approach retains expressivity through the mixtures encoded in the hierarchical model. For example, $L_1$ regularization may be represented as the MAP solution to a hierarchical model, for which the standard LASSO solution is produced using a coordinate ascent algorithm on the individual $x_i$ jointly with a local scale parameter. The order, and subsets of variables optimized simultaneously, are up to the designer to pick. Usually, these should be chosen to mirror the architecture of the hierarchical model.\n",
    "\n",
    "In our case, we seek to maximize the posterior distribution of $x, \\tau$ given $y$. We will, alternately, optimize $x$ given $\\tau$, then optimize $\\tau$ given $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f2adf-03b6-4424-ae50-6fe4ffcf9a7f",
   "metadata": {},
   "source": [
    "#### 1. Updating $x$\n",
    "\n",
    "In the first stage of coordinate ascent, we fix $\\tau$, and optimize over $x$.\n",
    "\n",
    "**Q 2.2:** Show that, finding the conditional MAP estimator of $x$ given $\\tau$ is equivalent to minimizing:\n",
    "\n",
    "$$f(x|\\tau,y) = \\| A x - y \\|^2 + \\frac{1}{2\\tau^2} \\|x\\|^2$$\n",
    "\n",
    "where $a_i$ is the $i$-th column of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f231ef-779e-4ad6-b3a0-60cb64234936",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161886a2-dc1d-4f45-8671-522bf88f7dfb",
   "metadata": {},
   "source": [
    "**Q 2.3:** Show that the conditional MAP estimator of $x$ given $\\tau$ is given by (check HW 3, or the normal, normal exercises in lab 3):\n",
    "\n",
    "$$\\hat{x}(\\tau,y) = \\left(A^{*} A + \\frac{1}{2 \\tau^2} I \\right)^{-1} A^{*} y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcd461-a2a9-42b1-aca9-397ca21917e5",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebab4b-17ce-4f36-8fd1-749b63212853",
   "metadata": {},
   "source": [
    "This suggests an automatic update step for $x$ given $\\tau$. If $\\{(x^{(t)},\\tau^{(t)})\\}_{t=0}^{...}$ is a sequence of guesses for the MAP estimator that iteratively approach the MAP via coordinate ascent, then, when updating the image $x$, given the scale $\\tau$, we should set:\n",
    "\n",
    "$$x^{(t+1)} = \\left(A^* A +  \\frac{1}{2 {\\tau^2}^{(t)}} I \\right)^{-1} A^* y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2daf5a-8baf-4f75-a493-8bb9162cee3b",
   "metadata": {},
   "source": [
    "#### 2. Updating $\\tau^2$\n",
    "\n",
    "In the second stage, we optimize over $\\tau$, holding $x$ fixed. That is, we aim to solve analytically for the conditional MAP estimator of $\\tau$ given $x$.\n",
    "\n",
    "**Q 2.4:** Using the structure of the hierarchical model, argue that the MAP estimator for $\\tau$ given $x$ is independent of $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ea05a-34a3-49e7-8e59-ed720098f6f0",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac6146-1d7b-41b9-9636-46324de5358e",
   "metadata": {},
   "source": [
    "**Q 2.5:** To find the MAP estimator for $\\tau$ given $x$, show that $\\tau^2|x$ has posterior density:\n",
    "\n",
    "$$p(\\tau^2 | x) \\propto (\\tau^2)^{-(\\alpha + n/2 + 1)} \\exp\\left(-\\frac{\\beta + 0.5 \\|x\\|^2}{\\tau^2}\\right)$$\n",
    "\n",
    "which is maximized at:\n",
    "\n",
    "$$\\hat{\\tau}^2_{\\text{MAP}}(x) = \\frac{\\beta + \\tfrac{1}{2} \\|x\\|^2}{\\alpha + \\tfrac{1}{2}n + 1}$$\n",
    "\n",
    "where $n$ is the number of entries in $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ffb10-d2d7-43fe-a38f-2de00e9985f5",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4879c-284c-4911-84c8-4617cd2a8714",
   "metadata": {},
   "source": [
    "**Q 2.6:** Refer back to Part I. Does this updating procedure match your intuition about how information should be passed through the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd4b30-79b6-4946-8905-c45a1cf88427",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc64d5a-957c-4824-b850-1a1089ab9537",
   "metadata": {},
   "source": [
    "This suggests the recursive step:\n",
    "\n",
    "$${\\tau^2}^{(t+1)} = \\frac{\\beta + \\tfrac{1}{2} \\|x^{(t+1)}\\|^2}{\\alpha + \\tfrac{1}{2}n + 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e08311-8692-4b7a-b25e-18fb293a2006",
   "metadata": {},
   "source": [
    "**Q 2.7:** How would this update step change if we adopted the second hierarchical model described in Part I (that is, used an indepndent scale parameter for each $x_i$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3b110-3000-4acc-afc5-efac6b3a24cd",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7169d6f-6e31-4836-8f23-0a3eb4d86479",
   "metadata": {},
   "source": [
    "#### 3. Implementation\n",
    "\n",
    "The full recursion is now:\n",
    "\n",
    "$$(x^{(t)},{\\tau^2}^{(t)}) \\longrightarrow x^{(t+1)} = \\left(A^* A +  \\frac{1}{2 {\\tau^2}^{(t)}} I \\right)^{-1} A^* y \\longrightarrow {\\tau^2}^{(t+1)} = \\frac{\\beta + \\tfrac{1}{2} \\|x^{(t+1)}\\|^2}{\\alpha + \\tfrac{1}{2}n + 1} \\longrightarrow (x^{(t+1)},{\\tau^2}^{(t)}) $$\n",
    "\n",
    "1. Implement coordinate ascent to iteratively update each variable $X_i$.\n",
    "2. Update the hyperparameter $\\tau^2$ using an Inverse-Gamma posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f4a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Coordinate Ascent\n",
    "def coordinate_ascent_fourier(A, y, alpha=2.0, beta=2.0, tol=1e-6, max_iter=300):\n",
    "    \"\"\"\n",
    "    Perform MAP estimation for Bayesian hierarchical model using Coordinate Ascent.\n",
    "    - X has a Gaussian prior: X_i ~ N(0, τ^2)\n",
    "    - τ^2 has an Inverse-Gamma hyperprior\n",
    "    \"\"\"\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros(n)  # Initialize X\n",
    "    tau2 = 0.5  # Initial value of tau2\n",
    "\n",
    "    AstarA = A.conj().T @ A\n",
    "    Astary = A.conj().T @ y\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        x_old = x.copy()  # Store previous x for convergence check\n",
    "\n",
    "        # The code below updates x using a coordinate ascent scheme.\n",
    "        #for i in range(n):\n",
    "        #    ai = A[:, i]\n",
    "        #    residual = Y - A @ X + ai * X[i]\n",
    "        #    X[i] = (ai @ residual) / (ai @ ai + 1/tau2 + 1e-6)  # Ensure numerical stability\n",
    "\n",
    "        # Update x given τ² (note: it is good numerical practice to avoid forming inverses. \n",
    "        # There are stabler and faster ways to solve linear systems. These are implemented in most solvers)\n",
    "        x = np.solve(AstarA + (1/(2*tau2))*np.eye(n), Astary)\n",
    "        \n",
    "        # Update τ² given x\n",
    "        tau2 = (beta + 0.5 * np.sum(X**2)) / (alpha + n/2 + 1)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(x - x_old) < tol:\n",
    "            break\n",
    "\n",
    "    return x, tau2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8fd628-8bdd-4a26-89ef-96b2724db5e6",
   "metadata": {},
   "source": [
    "### Gradient-Based Optimization (L-BFGS-B)\n",
    "\n",
    "Alternately, we could optimize the posterior with respect to $x$ and $\\tau$ using a standard gradient-based optimizer (e.g. gradient descent, Newton's method, etc).\n",
    "\n",
    "**Q 2.8:** Show that finding $\\hat{x}_{\\text{MAP}}(y),\\hat{\\tau}_{\\text{MAP}}^2(y)$ is the same as finding $\\hat{x}$, $\\hat{\\tau}^2$ that jointly maximize the objective:\n",
    "\n",
    "$$f(x, \\tau^2| y) = \\| A x - y \\|^2 + \\frac{1}{2\\tau^2} \\|x\\|^2+ \\alpha \\log(\\tau^2) + \\frac{\\beta}{\\tau^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4a3ba-78ce-4a7d-ab78-d5b38b851700",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d20147-4542-434b-8e81-940a3c103db9",
   "metadata": {},
   "source": [
    "**Q 2.9:** As is often true for hierarchical models built out of tractable parts, all derivatives of the log posterior are available in closed form. These can be passed as functions to an optimizer (e.g. pass a gradient and a Hessian for Newton type methods). As an example, compute the gradient of $f$ with respect to $x$ and $\\tau$ explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ceef5-6518-4678-b2db-23d256ce2c87",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3cafe4-2cd3-4c3b-85da-c2dbee474d23",
   "metadata": {},
   "source": [
    "The code below minimizes this objective with a standard, gradient-based optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f27f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_function(params, A, y, alpha=2.0, beta=2.0):\n",
    "    \"\"\"Objective function for MAP estimation using a gradient-based optimizer.\"\"\"\n",
    "    n = A.shape[1]\n",
    "    x = params[:-1]  # Extract X from parameter vector\n",
    "    tau2 = np.exp(params[-1])  # Ensure tau2 is positive by optimizing its log value\n",
    "\n",
    "    loss = np.linalg.norm(A @ x - y) ** 2 + np.sum(x**2) / (2 * tau2)\n",
    "    loss += alpha * np.log(tau2) + beta / tau2  # Inverse-Gamma prior on tau2\n",
    "    return loss\n",
    "\n",
    "# Optimize using L-BFGS-B\n",
    "def gradient_based_optimizer(A, y):\n",
    "    \"\"\"Optimize the MAP estimate using a gradient-based optimizer, jointly optimizing X and tau2.\"\"\"\n",
    "    n = A.shape[1]\n",
    "    x_init = np.zeros(n)\n",
    "    tau2_init = np.log(0.5)  # Optimize log(tau2) to enforce positivity\n",
    "\n",
    "    params_init = np.concatenate([x_init, [tau2_init]])\n",
    "\n",
    "    # Optimize both X and tau2\n",
    "    result = minimize(objective_function, params_init, args=(A, y), method='L-BFGS-B')\n",
    "    \n",
    "    x_opt = result.x[:-1]\n",
    "    tau2_opt = np.exp(result.x[-1])  # Convert back from log space\n",
    "    return x_opt, tau2_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835133ec",
   "metadata": {},
   "source": [
    "## Part III: Laplace Approximation for Uncertainty Estimation\n",
    "\n",
    "Suppose that we've run either coordinate-ascent or BFGS to convergence, and they have returned an estimator $\\hat{x}_{\\text{MAP}}(y), \\hat{\\tau}^2_{\\text{MAP}}(y)$. \n",
    "\n",
    "**Q 3.1:** Derive the Hessian of the log-posterior density about $\\hat{x}, \\hat{\\tau}$. Name it $H(\\hat{x},\\hat{\\tau})$, and show it only depends on the data $y$ through its arguments $\\hat{x}, \\hat{\\tau}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e360e-71c5-4e7c-9161-a4ce207a763f",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9209368-cb1f-4489-b81d-19c3fa7eef43",
   "metadata": {},
   "source": [
    "**Q 3.2:** Argue that, under a joint Laplace approximation to the posterior over $x$ and $\\tau$, the Laplace approximation to the marginal posterior distribution of $x$ (marginalizing over $\\tau$) is also normal, with covariance given by dropping one row and column from $H(\\hat{x},\\hat{\\tau})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23a8db-66b2-4000-ab76-25915e07109d",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d6236-c6f9-4c1e-beb4-5031dc746d52",
   "metadata": {},
   "source": [
    "**Q 3.3:** Complete the code below to return the covariance of the Laplace approximation to the posterior given a joint MAP estimator $\\hat{x}, \\hat{\\tau}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1f41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Laplace Approximation based on posterior\n",
    "def laplace_approximation_covariance(A, x_map, tau2, sigma2=1.0):\n",
    "    \"\"\"Compute the Laplace approximation to the posterior.\"\"\"\n",
    "    # compute the Hessian of the log-posterior\n",
    "    # invert to estimate the covariance (I'd suggest adding a small multiple of the identity to the Hessian before inverting for stability).\n",
    "    return covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12928822",
   "metadata": {},
   "source": [
    "## Part IV: Visualization and Comparison\n",
    "\n",
    "Let's compare our results. First, we will plot the MAP estimates for the images produced by both algorithms. Then, (*optional*) we will estimate the uncertainty in those estimates using our Laplace approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import inv\n",
    "\n",
    "# Compute MAP estimates using Bayesian hierarchical model with Fourier transform\n",
    "x_map_fourier, tau2_estimate = coordinate_ascent_fourier(A, y)\n",
    "x_map_gradient, tau2_gradient = gradient_based_optimizer(A, y)\n",
    "\n",
    "# Compute Laplace Approximation with posterior-based covariance\n",
    "x_map, covariance = laplace_approximation(A, x_map_fourier, tau2_estimate)\n",
    "\n",
    "# Reshape for visualization\n",
    "x_map_fourier_img = x_map_fourier.reshape(28, 28)\n",
    "x_map_gradient_img = x_map_gradient.reshape(28, 28)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "# Original MNIST Image\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img_array, cmap=\"gray\")\n",
    "plt.title(\"Original MNIST Image\")\n",
    "\n",
    "# Fourier Transformed Image (Log Magnitude for Better Visualization)\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(np.log(1 + np.abs(y.reshape(28, 28))), cmap=\"gray\")\n",
    "plt.title(\"Observed Image y\")\n",
    "\n",
    "# Reconstructed Image using Coordinate Ascent\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(x_map_fourier_img, cmap=\"gray\")\n",
    "plt.title(\"Reconstructed (Coord. Ascent)\")\n",
    "\n",
    "# Reconstructed Image using Gradient-Based Optimization\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(x_map_gradient_img, cmap=\"gray\")\n",
    "plt.title(\"Reconstructed (Grad-Based)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f47e2b-a022-4fdb-accc-4a8c4ba327ac",
   "metadata": {},
   "source": [
    "**Q 4.1:** In the space below report the MAP estimator for the nuisance parameter $\\tau^2$, and a 95\\% interval estimate for $\\tau^2$ based on the Laplace approximation to the posterior, for both optimization procedures. Are your answers consistent? If not, propose a reason the optimizers may have returned different answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf1b5f-51c8-4a61-94c5-741d081e41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a29877-c8e0-44d2-b142-c051f6f3923b",
   "metadata": {},
   "source": [
    "**Q 4.2:** *Optional.* To visualize the uncertainty in $x$, do the following:\n",
    "\n",
    "1. Make a heatmap image showing the standard deviation in each entry of $x$ using the Laplace approximation associated with each approximation to the MAP.\n",
    "2. Compute the SVD of the covariance of each Laplace approximation. The singular vectors of the covariance are the principal components. Make a series of 6 heatmap images, 3 per optimization method, each showing the 1st, 2nd, and 3rd principal components of the Laplace covariance. To do this, extract the first three singular vectors of the covariance, then reshape each to make an image. You may borrow some of the code written above to help visualize the results. Comment on the directions of principal uncertainty in the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc06519-6791-41a3-90ef-5750fc0cf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e5df6-8ed2-4615-a437-9c64c0a8adec",
   "metadata": {},
   "source": [
    "**Q 4.3:** Comment on the MAP estimators for the images and scale. If completed, the directions of principal posterior uncertainty about the estimator. How do you expect your results would change if you used a set of unknown local scale parameters $\\{\\tau_i\\}_{i=1}^{...}$ instead of a shared, global scale parameter? What signals $x$ would be promoted by this prior model? Would posterior estimation remain stable with twice as many unknowns as data points and as many nuisance parameters as parameters of inferential interest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc32f6d-b5c7-4cdf-864c-ebed268916c9",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096eb43a-3b43-42c4-9baa-ea5ae18a2814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
