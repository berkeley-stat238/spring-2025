{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc7b80c-6ded-4099-bb06-739fb77a1fab",
   "metadata": {},
   "source": [
    "# Lab 9: Hamiltonian Monte Carlo, Coverage Checks, and Posterior Predictive Checks with PyMC\n",
    "\n",
    "### Lab Date: Wednesday, Apr 16\n",
    "\n",
    "### Lab Due: Wednesday, Apr 30\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Work with your lab group to complete the following notebook. Your work will be reviewed by your peers in two weeks (Wednesday, April 30)\n",
    "\n",
    "In this lab, you will:\n",
    "1. **Sampling from an intractable posterior using Hamiltonian Monte Carlo (HMC):**  \n",
    "   Use PyMC to sample from a Bayesian logistic regression model where the posterior is analytically intractable.\n",
    "\n",
    "2. **Coverage:**\n",
    "    Check the accuracy of your posterior sampling procedure by performing a coverage check.\n",
    "\n",
    "4. **Posterior Predictive Checks (PPC):**  \n",
    "   Use the posterior samples to perform model checking.\n",
    "\n",
    "If you are new to working in python, or in a Jupyter notebook, please ask your lab members for help. If you notice a lab member struggling, and have experience, please offer your help.\n",
    "\n",
    "Please see this [Ed post](https://edstem.org/us/courses/74615/discussion/6463387) for corrections, questions, and discussion. If you would rather work with your own copy of the files, I have uploaded a zip folder there with the lab materials. \n",
    "\n",
    "Corrections to the lab will be pushed directly to this notebook. We will only push corrections to the text, which is set to read only to prevent merge conflicts. In the event of a merge conflict, save your notebook under a different name, and click the link that launches the lab from the schedule on the [stat238 homepage](https://stat238.berkeley.edu/spring-2025/) again. Then, check for discrepancies. If you can't find them, or resolve the conflict, contact us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4d5b3",
   "metadata": {},
   "source": [
    "## Problem Setup: Bayesian Logistic Regression\n",
    "\n",
    "We consider a binomial logistic regression model. For groups $j = 1, \\dots, J$, we observe:\n",
    "\n",
    "- $y_j \\sim \\text{Binomial}(n_j, \\theta_j)$\n",
    "- $\\theta_j = \\text{logistic}(\\alpha + \\beta x_j)$\n",
    "\n",
    "Here:\n",
    "- $x_j$ is a known covariate for group $j$\n",
    "- $\\alpha, \\beta$ are regression coefficients\n",
    "- $\\text{logistic}(z) = \\frac{1}{1 + e^{-z}}$ is the inverse logit function\n",
    "\n",
    "### Prior Distributions\n",
    "We place weakly informative priors:\n",
    "- $\\alpha \\sim t_4(0, 2^2)$  (Student-t with 4 degrees of freedom, mean 0, scale 2)\n",
    "- $\\beta \\sim t_4(0, 1)$\n",
    "\n",
    "These priors allow for heavier tails than the normal distribution, making them robust (c.f. lab 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc7594-39e8-4a36-930a-67a8089de520",
   "metadata": {},
   "source": [
    "## 1. Import Required Packages and Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ae3e25-8ec4-4dc7-b0cd-eb5a34a21099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import scipy.stats as st\n",
    "from scipy.special import expit  # logistic (inverse-logit) function\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35528a72",
   "metadata": {},
   "source": [
    "## 2. Simulate the Dataset\n",
    "\n",
    "**Q 2.1:** Simulate a dataset using the model:\n",
    "\n",
    "$y_j \\sim \\operatorname{Binomial}(n_j, \\operatorname{logit}^{-1}(\\alpha + \\beta\\, x_j))$\n",
    "\n",
    "where $J = 10$, the covariates $x_j$ are drawn from $\\text{Uniform}(-1, 1)$, and the sample sizes $n_j$ are drawn from a Poisson with $\\lambda = 5$ (reject any zeros). Use $\\alpha_{\\text{true}} = 0.5$ and $\\beta_{\\text{true}} = -1.0$ for simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88458e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 0.39 -0.43 -0.55  0.1   0.44 -0.15  0.96  0.37 -0.04 -0.22]\n",
      "n: [3 4 8 4 8 7 5 9 4 3]\n",
      "y: [1 3 7 3 3 5 2 5 3 1]\n"
     ]
    }
   ],
   "source": [
    "# Set simulation parameters\n",
    "J = 10\n",
    "alpha_true = 0.5\n",
    "beta_true = -1.0\n",
    "\n",
    "# Draw x_j from Uniform(-1, 1)\n",
    "x = np.random.uniform(-1, 1, size=J)\n",
    "\n",
    "# Draw n_j from a truncated Poisson (reject zeros)\n",
    "n = np.empty(J, dtype=int)\n",
    "for j in range(J):\n",
    "    nj = 0\n",
    "    while nj < 1:\n",
    "        nj = np.random.poisson(lam=5)\n",
    "    n[j] = nj\n",
    "\n",
    "# Compute the probabilities theta_j = logistic(alpha_true + beta_true * x_j)\n",
    "theta = ... # complete this line\n",
    "\n",
    "# Draw y_j ~ Binomial(n_j, theta_j)\n",
    "y = ... # complete this line\n",
    "\n",
    "print(\"x:\", np.round(x, 2))\n",
    "print(\"n:\", n)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c99541-fb74-4581-9291-505038f93f24",
   "metadata": {},
   "source": [
    "## 3. Build and Sample the Model in PyMC\n",
    "\n",
    "PyMC is a powerful, open-source library for probabilistic programming in Python that simplifies the process of building and fitting Bayesian models. It provides an intuitive interface for defining complex statistical models and performing inference using state-of-the-art sampling algorithms such as NUTS. If you're new to PyMC or Bayesian modeling, check out the <a href=\"https://docs.pymc.io/\" style=\"color: blue;\">official PyMC documentation</a> for tutorials, examples (e.g. <a href=\"https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html#glm-linear\" style=\"color: blue;\">GLM Linear Regression</a>), and comprehensive guidance.\n",
    "\n",
    "If you get stuck in the documentation, check with the TA during lab or ask for help in office hours.\n",
    "\n",
    "**Q 3.1:** In the space below:\n",
    "\n",
    "- Define the Bayesian model using PyMC.\n",
    "- Use Studentâ€‘t priors with 4 degrees of freedom for $\\alpha$ (scale = 2) and $\\beta$ (scale = 1).\n",
    "- Define the likelihood: $y_j \\sim \\operatorname{Binomial}(n_j, \\theta_j)$ with $\\theta_j = \\operatorname{logit}^{-1}(\\alpha + \\beta\\, x_j)$.\n",
    "- Sample from the posterior using the No-U-Turn Sampler (NUTS), a type of HMC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors for alpha and beta using Student-t distributions\n",
    "    alpha = pm.StudentT(\"alpha\", nu=4, mu=0, sigma=2) # an example call to pymc defining a distributional model component\n",
    "    beta = ...\n",
    "    \n",
    "    # Logistic regression: theta = logit^{-1}(alpha + beta * x)\n",
    "    theta = ... # use the pm.Deterministic function to fix a deterministic model component\n",
    "    \n",
    "    # Likelihood: y_j ~ Binomial(n_j, theta_j)\n",
    "    y_obs = ... # use the observed = argument to fix the observation\n",
    "    \n",
    "    # Sample using NUTS (HMC)\n",
    "    sample_trace = ... # use the pm.sample function\n",
    "\n",
    "# Display the posterior summary for alpha and beta\n",
    "print(az.summary(sample_trace, var_names=[\"alpha\", \"beta\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd258de0-2d00-4c6e-a1e4-1eb5294c6f2b",
   "metadata": {},
   "source": [
    "**Q 3.2:** Adjust the sampling parameters (primarily, the number of steps taken) until you are convinced that the chain has mixed, and that you have enough samples to reliably represent the posterior. Explain the criteria you used in the space below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c0acc-905a-432c-aa41-2056d0ff341c",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea122e-3cf6-4255-be2f-c44fcbae67a2",
   "metadata": {},
   "source": [
    "## 4. Coverage\n",
    "\n",
    "**Q 4.1:** Do your 50% posterior intervals for $\\alpha$ and $\\beta$ (25th to 75th percentiles) contain the true values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29550c-b2c2-4335-bb7e-0183b53c3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0007387-cfb8-49ac-ac60-13f09cc727f9",
   "metadata": {},
   "source": [
    "Our posterior inferences are approximate since they depend on an MCMC procedure which only samples from the exact posterior given infinitely long to run. To check that your sampling procedure reliably represents the true posterior, perform a coverage check.\n",
    "\n",
    "**Q 4.2:** That is:\n",
    "\n",
    "1. Generate a series of $m$ replicate data sets sampled from the full data generating model (sample $\\alpha, \\beta$ from the prior, resample $x$ and $n$ using 10 observations as before (alternately, you can check coverage conditional on the same $x$ and $n$ from the main test case), then sample a series of new $y$'s.\n",
    "2. For each, run your sampling code to generate approximate samples from the posterior. For each set of samples build 65%, 80%, 90%, and 95% credible intervals for $\\alpha$ and $\\beta$.\n",
    "3. Track the fraction of replicates for which the sampled $\\alpha$ and $\\beta$ land in each posterior interval. Return a table comparing the fraction of the time that the replicates should have been contained in the interval (e.g. 65% for the 65% intervals), and the fraction that actually were contained inside the intervals. Choose $m$ large enough so that the observed fractions are sufficiently stable for evaluation.\n",
    "4. Report the expected standard deviation in the observed fraction of posterior intervals containing the truth. If our posterior sampling procedure is exact, and we use an interval that is chosen to contain the truth $p$ percent of the time, and we run $m$ replicates, then the observed percent of replicates containing the truth is $C/m$ where $C \\sim \\text{Binomial}(m,p)$. Use the standard deviation of $C/m$ for each $p$ to choose $m$.\n",
    "5. Based on steps 3 and 4, evaluate whether the discrepancy between the observed fraction contained in each interval, and the predicted fraction, is plausibly explained by randomness in the trials, or is evidence of systematic error in your posterior sampling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea22bc4-add8-49ab-bfe1-43339f96970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50c4c4-b07c-4c93-acb0-8a5942a58a05",
   "metadata": {},
   "source": [
    "**Q 4.3:** In the space below, reflect on the role of coverage checking in a Bayesian analysis when we don't know the true data generating model. Does a coverage check validate the model? If not, what aspect of the Bayesian process is validated using coverage checks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323af30-59e1-4848-9ebb-d49865cd5616",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c866435-6416-48a8-898b-bd879d4c6b6f",
   "metadata": {},
   "source": [
    "**Q 4.4:** In the space below, explain why we need many replicates from the data generating model to perform a coverage check (why wasn't Q4.1 sufficient by itself)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96751761-d85a-47db-b2b8-df4efff3e2f2",
   "metadata": {},
   "source": [
    "*Write your answere here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c00307-e009-4ca8-af70-8ca838d25c35",
   "metadata": {},
   "source": [
    "## 5. Posterior Predictive Checks (PPC) and Plotting the Fit\n",
    "\n",
    "Now, generate posterior predictive samples to perform model checking. \n",
    "\n",
    "**Q 5.1:** In the space below:\n",
    "\n",
    "- Generate $m$ posterior predictive samples conditional on the observed data, sample sizes $n$ and covariates $x$. That is, resample $\\alpha, \\beta$ from the posterior (use your posterior samples), then generate replicate $y's$ given $\\alpha, \\beta, x$ and $n$.\n",
    "- Create a plot comparing the proportions $y_j/n_j$ in the observed data set and the proportions predicted by replication. I would suggest a scatter plot whose horizontal coordinate is the observed proportion, and whose vertical coordinate is the replicated proportion for each replicate. For each $j$, compute an interval that contains 95% of the replicate proportions, then overlay the interval on your scatter plot. Finally, add the line of parity (line of slope one) to check agreement.\n",
    "\n",
    "Vary $m$ until you believe you have enough replicates to answer the question, \"could my data have plausibly been generated by this model?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75b83f-ccc3-4246-bed0-53c6f80132f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here (it may help to use the pm.sample_posterior_predictive function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7931fe-68d4-4183-892a-850d5bddecbd",
   "metadata": {},
   "source": [
    "**Q 5.2:** To show that a PPC can reject mispsecified models, regenerate the data and rerun the PPC, but use a different model for generating the data than you do for posterior inference (e.g. try using two different prior parameters, one when you sample the data, and one when you run posterior inference). Vary the models until your PPC procedure would reject the model you used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da736478-6713-4836-accd-c7db277b32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here (it may help to use the pm.sample_posterior_predictive function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302de641-d77b-4b00-9e94-c3fe3b1bd40e",
   "metadata": {},
   "source": [
    "**Q 5.3:** Explain how you would formalize the graphical PPC described above as a hypothesis test, and, based on your experience in Q5.2 whether your PPC procedure would make a sensitive (e.g. a powerful) test for model misspecification with respect to the parameter you chose. Explain why you believe your procedure would form a sensitive, or insensitive, test for the chosen parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39ae85-813a-4796-98ec-7df2f53af875",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fef56f-0abe-462b-b99d-113f30a2203c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
