{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdda84b0",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "### Lab Date: Wednesday, January 29\n",
    "\n",
    "### Due: Wednesday, February 5\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Work with your lab group to complete the following notebook. It will be reviewed by your peers in lab next week (Wednesday, February 5th). \n",
    "\n",
    "This notebook is only lightly scaffolded. This is intentional - as the learning goal for today's lab is as much about how to frame prior fitting and model evaluation as is it the necessary computation. As such, I have left the two major questions at the end of the lab open. If your group is stuck and unsure how to proceed, ask the instructor during lab, come into OH, or, review Chapter 6. Many of the ideas in Chapter 6 are essentially standard statistical practice, wrapped around a Bayesian pipeline, so can be discovered independently without much technical course knowledge. This lab is designed to give you enough space to discover some of these ideas.\n",
    "\n",
    "If you are new to working in python, or in a Jupyter notebook, please ask your lab members for help. If you notice a lab member struggling, and have experience, please offer your help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b4cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Set Up\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy as scipy\n",
    "from scipy import io, integrate, linalg, signal\n",
    "import pandas as pd\n",
    "# import pymc as pm\n",
    "# import bambi as bmb\n",
    "# import arviz as az\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add what you like beneath here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c454244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data for the Lab\n",
    "success_counts = pd.read_csv('Beta_Binomial_Draws',header=None) # The first column is the number of trials, the second is the number of successes\n",
    "success_string = pd.read_csv('Ys',header=None) # The specific outcome string (sequence of 0, 1's) for the 100th trial\n",
    "true_probabilities = pd.read_csv('Thetas_true',header=None) # The true outcome probabilities for each row. DO NOT LOOK AT THIS UNTIL THE END. It is \"unknown\" and in most problems, unknowable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1849401",
   "metadata": {},
   "source": [
    "## The (Beta, Binomial) Model\n",
    "\n",
    "As in class, and as in Chapter 2.4, consider the (Beta, Binomial) model. That is, we draw a success probability $\\Theta$ from a Beta distribution with parameters $\\alpha, \\beta$, then draw $S$ successes from a Binomial distribution on $n$ trials, with success probability $\\Theta$:\n",
    "\n",
    "$$ \\Theta \\sim \\text{Beta}(\\alpha,\\beta), \\quad S \\sim \\text{Binomial}(n,\\Theta)$$\n",
    "\n",
    "We will assume, as usual, that $n$ is known, $S$ is observed, and all other variables are unknown.\n",
    "\n",
    "It will be our goal in this lab to practice building posterior distributions, and posterior estimates to the unknown success probabilities. We learned how to do this in class for any fixed $\\alpha, \\beta$. Your main goal for this lab is to learn how to reasonably estimate your prior parameters (note, I did not give them to you), and to check that your prior model leads to sensible posterior inference (model checking). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf65dc",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "### The Observables\n",
    "\n",
    "The file loaded into `success_counts` contains 100 independent draws from this model, for varying $n$. The first column contains the number of trials, the second contains the number of success.\n",
    "\n",
    "The file loaded into `success_string` contains the actual sequence of outcomes for the 100th trial.\n",
    "\n",
    "### The Unknown\n",
    "\n",
    "The file loaded into `true_probabilities` contains the true probabilities $\\{\\Theta_i\\}_{i=1}^{100}$ for the 100 draws from the full joint model on $(\\Theta, S)$. Do not look at this until the end, or unless you are absolutely out of ideas. I have made it available here so that, after choosing your inferential methods, and evaluation scheme, you can go back and check whether we got close to the truth. You should avoid looking at this since in essentially any real Bayesian setting, this would never be known. It is common when testing pipelines to generate and save the truth, as we have done here, to validate that the pipeline is accurate had you known the truth. We'll save this evaluation for the end. For most of the lab, operate under the assumption that this file is inaccessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb24ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to explore the observable data. Make whatever simple EDA plots you think you need to get some feel for it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700802c1",
   "metadata": {},
   "source": [
    "## The Inferential Pipeline\n",
    "\n",
    "In class we discussed how to derive the posterior distribution for a model with a Beta prior and a Binomial likelihood. In particular, since the models are conjugate, the posterior is also a Beta distribution:\n",
    "\n",
    "$$\\Theta|S = s ; n, \\alpha, \\beta \\sim \\text{Beta}(s + \\alpha, n - s + \\beta)$$\n",
    "\n",
    "\n",
    "### Your aim:\n",
    "\n",
    "We want to estimate $\\Theta_{100}$ from $n_{100}$ and $S_{100}$. The other 99 draws from the joint model have been provided for your use either as prior data (note that $\\alpha, \\beta$ are the same for all 100 draws), or for model checking.\n",
    "\n",
    "**In the cell below:** write a function that accepts $s, n, \\alpha, \\beta$ and returns the following posterior summaries:\n",
    "1. The MLE estimator: $\\hat{\\theta}_{\\text{MLE}}(s;n,\\alpha,\\beta)$\n",
    "1. The posterior mean: $\\hat{\\theta}_{\\text{mean}}(s;n,\\alpha,\\beta)$\n",
    "1. The posterior mode: $\\hat{\\theta}_{\\text{MAP}}(s;n,\\alpha,\\beta)$\n",
    "1. The posterior standard deviation: $\\text{SD}[\\Theta|S=s]$\n",
    "1. A basic interval estimate for $\\Theta|S=s$ using posterior summaries (1. - 3.). Choose your interval estimate so that it should contain the truth with reasonably high probability. You may decide as a group what convention to use here (a Chebyshev bound, the normal approximation to the Beta, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63ca11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6814bdc",
   "metadata": {},
   "source": [
    "**In the cell below:** write a function that accepts $s, n, \\alpha, \\beta$ and returns $m$ i.i.d. samples from the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55084e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63c830bf",
   "metadata": {},
   "source": [
    "**In the space below:** write a code that accepts $s,n,\\alpha,\\beta$, a chosen number of posterior samples $m$, and a coverage probability $p$. Your code should return:\n",
    "1. Returns a plot of the posterior density overlaid on a histogram of the $m$ samples\n",
    "1. Print an interval estimate for $\\Theta|S=s$ that contains the unknown with probability $p$. Your interval may be estimated from samples (as is usually done for more complex models), or, as the Beta CDF is easy to work out analytically, from exact quantiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b26d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9860eb6",
   "metadata": {},
   "source": [
    "**In the space below:** Apply the codes you wrote above to explore the range of different posterior distributions for $\\Theta_{100}|S_{100} = s_{100}$ that can be produced by varying the prior parameters $\\alpha$ and $\\beta$. I am not going to give you specific prior parameter pairs to check here. Instead, be responsible for your own exploration. Stop once you think you have sufficiently demonstrated the possible posteriors, and their dependence on the possible prior assumptions.\n",
    "\n",
    "*Note: For this sort of demonstration, it helps to do a little back of the envelope thinking first about how the prior parameters bias the posterior, and the balance of information provided by the prior and by the data collected.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b3303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "845a43d5",
   "metadata": {},
   "source": [
    "## Gathering Information\n",
    "\n",
    "In Bayesian statistics, observations provide information via conditioning. Let's see how our posterior changes sequentially as we gain evidence.\n",
    "\n",
    "**In the cell below:** Write a code that walks through the `success_string` corresponding to the 100th trial. As it goes, use the code you wrote above to print out a sequence of plots and interval estimates for the unknown. For now, initialize the process with a uniform prior ($\\alpha = 1, \\beta = 1$). What do you notice about the posterior (its shape, its uncertainty with respect to resampling, etc.) as we gather evidence? What do you notice about its point summaries (the MLE, MAP, and posterior mode)?\n",
    "\n",
    "*Note: you don't need a unique output for all 70 trials in the success string. Instead, space your outputs to show the main trends. You should space your outputs tighter at the beginning, when our posterior is more sensitive to any individual trial outcome, and less tightly at the end. As a basic heuristic, the spacing should grow linearly so that the sampled trial lengths follow a quadratic sequence (e.g. 1, 4, 9, 16, 25, 36, 64).*\n",
    "\n",
    "If you feel you already have a decent intuition for the Beta distribution, I would save this step for the end of the lab. If you'd like to try a basic widget to see how the shape of the Beta depends on its parameters, go to [this article](https://www.mathmouth.com/bayesball) and scroll to Demo 3 (roughly 3/5ths of the way down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c640f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4b003fb",
   "metadata": {},
   "source": [
    "## Fitting the Prior\n",
    "\n",
    "You should have seen that your estimates above depend on your choice of prior parameters. This is the point of Bayesian inference. The prior *should* influence our estimation.\n",
    "\n",
    "In order to influence our estimates informatively, the prior must encode actual information. Otherwise, the Bayesian pipeline is really just a different estimation procedure that uses the mathematics of conditioning to define intervals and to regularize the inference.\n",
    "\n",
    "Discuss with your group how you could use the first 99 sample outcomes to estimate $\\alpha$ and $\\beta$. \n",
    "\n",
    "*For now, we will assume that the form of the prior model (i.e. the Beta) is correct. We will discuss what happens if your prior form is misspecified (does not contain the true data generating distribution for any parameter values) later in the class. I generated this data using the model specified above.*\n",
    "\n",
    "If you get stuck, consider the following pair of ideas:\n",
    "1. You could select a set of previous observations with large $n_j$, estimate $\\theta_j$ for each with a simple point estimator, then fit the resulting ensemble of estimated $\\theta$'s to a Beta distribution (either by matching moments or via an MLE). *If you choose this route, discuss the trade-off between using many past samples, and incorporating less reliable past samples*\n",
    "1. Consider the marginal distribution of $S$ drawn from the joint model $(\\Theta, S)$. When evaluated at $S = s$ we called this the \"evidence\". Discuss the relationship between the marginal distribution of $\\{S_j\\}_{j=1}^{99}$ and the prior parameters. Does this relation have a familiar name? Could you use it to estimate $\\alpha$ and $\\beta$? If so, what standard framework could you adopt?\n",
    "\n",
    "How you proceed from here is up to you. Note: if you choose 1. you will have to explain how you fit (i.e. how to solve the moment matching problem). If you choose 2, I strongly suggest you look ahead to the first part of the \"Model Checking\" section of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071382b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d76f685",
   "metadata": {},
   "source": [
    "Comment on your degree of confidence in your prior parameter fits. Think about simple procedures for evaluating your uncertainty in your fits with your group. Based on your experience toying with the posterior for different prior choices, do you believe you've resolved the parameters with enough confidence so that your posterior is reliable (i.e. the uncertainty captured in the posterior fully captures your uncertainty about the unknown)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ae4db",
   "metadata": {},
   "source": [
    "*Replace this text with your discussion. You are welcome to attempt the procedure you sketch out below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25992a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ecb57b6",
   "metadata": {},
   "source": [
    "## Model Checking\n",
    "\n",
    "### The Evidence\n",
    "\n",
    "Derive the marginal distribution for the number of observed successes $S$ given $(\\Theta,S)$ drawn with parameters $\\alpha, \\beta, n$. That is, derive an explicit formula for $\\text{Pr}(S = s)$ for all $s \\in \\{1,2,...,n\\}$. You may express your answer using any standard combinatorial functions (factorial, choose, $\\Gamma$, Beta, etc). Do not try to close the integrals directly. Instead, reference the normalization factor used in the Beta distribution's density function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab8e96",
   "metadata": {},
   "source": [
    "*Replace this text with your analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae29e6",
   "metadata": {},
   "source": [
    "Give an argument justifying the word \"evidence\" to describe $\\text{Pr}(S = s)$. Relate your answer to the likelihood function of the prior parameters $\\alpha, \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abaac3f",
   "metadata": {},
   "source": [
    "*Replace this text with your discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca2e9d",
   "metadata": {},
   "source": [
    "### Posterior Predictive Distribution\n",
    "\n",
    "Use your work above, and the rule that updates the prior to produce the posterior, to derive the posterior predictive distribution for a new set of $m$ sample outcomes, given an observed set of $n$ outcomes containing $s$ successes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73f5ef",
   "metadata": {},
   "source": [
    "*Replace this text with your analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706eaa7c",
   "metadata": {},
   "source": [
    "With your group, discuss how you could use this distribution to check whether your model (and, deductively, your inferences regarding the unknown) are realistic given your observations.\n",
    "\n",
    "Hint:\n",
    "1. It may help to first think about trying to evaluate the probability of observing what you observed, conditioned on what you observed. This will motivate the need for the word \"prediction\", and the distinction between evaluating a likelihood, evaluating the evidence, and predictive checking.\n",
    "1. Since we only have access to a fixed data set, consider breaking your data set into pieces that serve separate purposes. Just as we secluded a prior data set ($j = \\{1, 2, ..., 99\\}$), we could further split the data to include an evaluation or test set. Discuss how you might compare posterior predictions to this test set, whether it would make sense to evaluate the posterior predictive probability of observing the test set, and what does or doesn't go wrong if you tried to evaluate the posterior predictive probability of observing the data *that you did observe (i.e. conditioned on).*\n",
    "\n",
    "If you start going in circles here, call over the instructor, or review BDA chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409396b",
   "metadata": {},
   "source": [
    "*Replace this text with your discussion. Then, describe your method, and implement an example below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b62a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8343719",
   "metadata": {},
   "source": [
    "## Post-Script (Peaking at the Answer)\n",
    "\n",
    "*This part is optional.*\n",
    "\n",
    "Now that you have a pipeline for: (a) fitting the prior, (b) using it to produce inferences, and (c) rejecting models that produce implausible inferences, it's worth seeing whether that pipeline typically returns accurate inferences. \n",
    "\n",
    "Select a subset of the provided data as a prior set for fitting the prior parameters, then apply steps (b) and (c) to the remaining test cases. Think carefully about whether you should use step (c) to eliminate specific test cases, or to eliminate all inferences if enough test cases are sufficiently implausible. Then, compare your remaining answers to the true success probabilities.\n",
    "\n",
    "Note: A nice way to summarize whether your pipeline is producing accurate answers when you have access to the truth is via a coverage test. That is, return an interval estimate for each unknown $\\Theta_j$ in the test set that should hold the unknown with some large success probability (I usually repeat this for success probabilities 0.5, 0.75, 0.875, 0.95, 0.99 or something to the effect). Then, compute the fraction of the intervals that *actually* contained the truth. These should match. Notice that this is essentially a procedural guarantee. Consider the analogy to confidence intervals. This process of checking whether the pipeline produces intervals that actually cover the unknown as often as they purport to cover the unknown is especially important in more complicated examples when we can't perform step (b) exactly, and instead use approximations to perform posterior inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beecad10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
